{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/getachew_abebe/LLM_Fine-Tunning_Amharic/llm_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "from utils import calculate_length, preprocess_article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "2d8601d8fa6949dd90a7555108e82f7e"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-05-30T14:20:22.516890Z",
          "iopub.status.busy": "2024-05-30T14:20:22.515888Z",
          "iopub.status.idle": "2024-05-30T14:20:32.271734Z",
          "shell.execute_reply": "2024-05-30T14:20:32.270988Z",
          "shell.execute_reply.started": "2024-05-30T14:20:22.516854Z"
        },
        "id": "H-AfF9KLiJnz",
        "outputId": "3dcee90a-4864-4339-9871-af30c5d7b3c1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "data = load_dataset(\"csv\", data_files=\"/home/getachew_abebe/LLM_Fine-Tunning_Amharic/data/Amharic.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "data['train'] = data['train'].map(calculate_length, batched=False)\n",
        "data['train'] = data['train'].map(preprocess_article, batched=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model\n",
        "base_model = \"NousResearch/Llama-2-7b-hf\"\n",
        "new_model = \"llama-2-7b-Amharic\"\n",
        "# Dataset\n",
        "dataset = data\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.94s/it]\n",
            "/home/getachew_abebe/LLM_Fine-Tunning_Amharic/llm_env/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/home/getachew_abebe/LLM_Fine-Tunning_Amharic/llm_env/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/home/getachew_abebe/LLM_Fine-Tunning_Amharic/llm_env/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/home/getachew_abebe/LLM_Fine-Tunning_Amharic/llm_env/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
        ")\n",
        "\n",
        "# Load base moodel\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "\n",
        "# Cast the layernorm in fp32, make output embedding layer require grads, add the upcasting of the lmhead to fp32\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['train']\n"
          ]
        }
      ],
      "source": [
        "print(list(dataset.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-30T14:20:32.274101Z",
          "iopub.status.busy": "2024-05-30T14:20:32.273664Z",
          "iopub.status.idle": "2024-05-30T14:20:32.280714Z",
          "shell.execute_reply": "2024-05-30T14:20:32.279815Z",
          "shell.execute_reply.started": "2024-05-30T14:20:32.274075Z"
        },
        "id": "pvVE6jzxiJn-",
        "outputId": "81341fb5-8427-44a3-f9cd-ac2a4a519749",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/getachew_abebe/LLM_Fine-Tunning_Amharic/llm_env/lib/python3.8/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "/home/getachew_abebe/LLM_Fine-Tunning_Amharic/llm_env/lib/python3.8/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "/home/getachew_abebe/LLM_Fine-Tunning_Amharic/llm_env/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 : < :, Epoch 0.00/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Set training arguments\n",
        "training_arguments = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=10,\n",
        "        gradient_accumulation_steps=1,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=1000,\n",
        "        logging_steps=1,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        learning_rate=2e-4,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        warmup_steps=10,\n",
        "        # report_to=\"wandb\",\n",
        "        max_steps=2, # Remove this line for a real fine-tuning\n",
        "        logging_strategy=\"steps\",  # <--- Add this line\n",
        "        save_strategy=\"steps\",  # <--- Add this line\n",
        "\n",
        ")\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['train'],\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"article\",\n",
        "    max_seq_length=512,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "# # Access the training and evaluation losses\n",
        "# train_loss = trainer.state.log_history[-1]['train_loss']  # Last training loss\n",
        "# eval_loss = trainer.state.log_history[-1]['eval_loss']   # Last evaluation loss\n",
        "\n",
        "# print(f\"Training Loss: {train_loss}\")\n",
        "# print(f\"Evaluation Loss: {eval_loss}\")\n",
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'DatasetDict' object has no attribute 'describe'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescribe\u001b[49m()\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DatasetDict' object has no attribute 'describe'"
          ]
        }
      ],
      "source": [
        "dataset['article']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-30T14:21:25.789869Z",
          "iopub.status.busy": "2024-05-30T14:21:25.788920Z",
          "iopub.status.idle": "2024-05-30T14:21:26.010834Z",
          "shell.execute_reply": "2024-05-30T14:21:26.009892Z",
          "shell.execute_reply.started": "2024-05-30T14:21:25.789832Z"
        },
        "id": "qZc1187yiJoB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "category = list(set(data['train']['category']))\n",
        "\n",
        "checkpoint = \"meta-llama/Llama-2-7b-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Tokenize the dataset\n",
        "category_to_id = {cat: idx for idx, cat in enumerate(category)}\n",
        "\n",
        "def tokenize_function(example):\n",
        "    inputs = tokenizer(example['article'], padding=True, truncation=True, max_length=512)\n",
        "    inputs[\"labels\"] = category_to_id[example[\"category\"]]  # Assuming category is already integer-encoded\n",
        "    return inputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "bca72e4e593d4a1382d2d9b16fe37eaf",
            "f0073336b200468f9e14d05d0be56b2b",
            "5f82fce25f494425926be0ab0e5183a0",
            "e16efbd58a6241169acd66631268dd13",
            "8ceba03ca79941a398ad5657bbe765c7",
            "48a07e19134e4a1eb7040aa02bbfedcb"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-05-30T14:21:26.012250Z",
          "iopub.status.busy": "2024-05-30T14:21:26.011969Z",
          "iopub.status.idle": "2024-05-30T14:24:07.089243Z",
          "shell.execute_reply": "2024-05-30T14:24:07.088286Z",
          "shell.execute_reply.started": "2024-05-30T14:21:26.012219Z"
        },
        "id": "-6TPC4usiJoC",
        "outputId": "40af8d5f-47b4-4c22-9c8d-3ad6992cb43a",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49532/49532 [03:15<00:00, 253.63 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12383/12383 [00:48<00:00, 254.55 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['article', 'category', 'word_count', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 49532\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['article', 'category', 'word_count', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 12383\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tokenized_datasets = raw_datasets.map(tokenize_function)\n",
        "# Use a data collator to apply dynamic batches\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors='pt')\n",
        "\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "print(tokenized_datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'article': 'áˆ¶áˆ›áˆá‹« á‹¨áˆšáŒˆáŠ˜á‹áŠ“ áˆžá‰…á‹²áˆ¾ áˆµá‰³á‹²á‹¨áˆ á‹áˆµáŒ¥ áˆ°ááˆ® á‹¨áŠá‰ áˆ¨á‹ á‹¨á‰°áˆ˜á‹µ áˆšáˆ½áŠ• áŠ áˆšáˆ¶áˆ áŠ¨áˆµá–áˆ­á‰µ áˆœá‹³á‹ áˆˆá‰† áˆ˜á‹áŒ£á‰± á‰°áŒˆáˆˆá€áŠ ááˆªáŠ« áˆ…á‰¥áˆ¨á‰µ áˆšáˆ½áŠ• á‰ áˆ˜áŒ¨áˆ¨áˆ» á‹¨á‹ˆáŒ£á‰±áŠ• á‹µáˆá… áˆ°áˆá‰¶ á‰¥áˆ„áˆ«á‹Šá‹ áˆµá‰³á‹²á‹¨áˆ áŠ¥áŠ•á‹°áŒˆáŠ“ áˆˆáˆµá–áˆ­á‰± áŠ¥áŠ•á‰…áˆµá‰ƒáˆ´ áŠ¥áŠ•á‹²á‹áˆ áˆœá‹³á‹áŠ• á‰ áˆ˜áˆá‰€á‰ áŠ¥áŒ…áŒ á‰°á‹°áˆµá‰°áŠ“áˆ áˆ²áˆ‰ á‹¨á‹ˆáŒ£á‰¶á‰½áŠ“ áˆµá–áˆ­á‰µ áˆšáŠ’áˆµá‰µáˆ¯ áŠ¨áˆ€á‹²áŒ† áˆžáˆ€áˆ˜á‹µ á‰ áˆ­áŠ­áŠ­á‰¡ áˆµáŠ áˆµáˆ­áŠ á‰µ áˆ‹á‹­ á‰°áŠ“áŒáˆ¨á‹‹áˆá‹¨áˆ¶áˆ›áˆá‹«á‹ á•áˆ¬á‹šá‹°áŠ•á‰µ áˆžáˆ€áˆ˜á‹µ áŠ á‰¥á‹±áˆ‹áˆ‚ áŽáˆ­áˆ›áŒ† á‰ á‰ áŠ©áˆ‹á‰¸á‹ áˆ˜áŠ•áŒáˆµá‰µ á‹¨áŒ¦áˆ­ á‰€áŒ£áŠ“ áˆ†áŠ– á‹¨á‰†á‹¨á‹áŠ•áŠ“ á‰ áŠ¥áŒ…áŒ‰ á‹¨á‰°áŒŽá‹³á‹áŠ• áˆµá‰³á‹²á‹¨áˆ áˆˆáˆ€áŒˆáˆªá‰± á‹¨áˆµá–áˆ­á‰µ áŠ¥áŠ•á‰…áˆµá‰ƒáˆ´ áˆˆáˆ›á‹‹áˆ áŒ¥áˆ¨á‰µ áŠ¥áŠ•á‹°áˆšá‹«á‹°áˆ­áŒ áŠ áˆµá‰³á‹á‰€á‹‹áˆá‹­áˆ… áŠ¥áŠ áŠ  á‰ 1970á‹Žá‰¹ á‰ á‰»á‹­áŠ“á‹á‹«áŠ• á‹¨á‰°áŒˆáŠá‰£á‹ á‰³á‹²á‹¨áˆ áŠ¨á‹ˆá‰³á‹°áˆ«á‹Š áˆáˆáˆá‹µ áˆŒáˆ‹ áˆáŠ•áˆ áŠ á‹­áŠá‰µ á‹¨áŠ á‰µáˆŒá‰²áŠ­áˆµ áŠ¥áŠ•á‰…áˆµá‰ƒáˆ´ áŠ¥áŠ•á‹³áˆ‹áˆµá‰°áŠ“áŒˆá‹° á‹­á‰³á‹ˆá‰ƒáˆ',\n",
              " 'category': 'International News',\n",
              " 'word_count': tensor(443),\n",
              " 'input_ids': tensor([    1, 29871,   228,   139,   185,   228,   139,   158,   228,   139,\n",
              "           144,   228,   142,   174, 29871,   228,   142,   171,   228,   139,\n",
              "           157,   228,   143,   139,   228,   141,   155,   228,   142,   144,\n",
              "           228,   141,   150, 29871,   228,   139,   161,   228,   140,   136,\n",
              "           228,   142,   181,   228,   139,   193, 29871,   228,   139,   184,\n",
              "           228,   140,   182,   228,   142,   181,   228,   142,   171,   228,\n",
              "           139,   160, 29871,   228,   142,   144,   228,   139,   184,   228,\n",
              "           143,   168, 29871,   228,   139,   179,   228,   144,   144,   228,\n",
              "           139,   177, 29871,   228,   142,   171,   228,   141,   147,   228,\n",
              "           140,   163,   228,   139,   171,   228,   142,   144, 29871,   228,\n",
              "           142,   171,   228,   140,   179,   228,   139,   155,   228,   142,\n",
              "           184, 29871,   228,   139,   157,   228,   139,   192,   228,   141,\n",
              "           152, 29871,   228,   141,   163,   228,   139,   157,   228,   139,\n",
              "           185,   228,   139,   160, 29871,   228,   141,   171,   228,   139,\n",
              "           184,   228,   144,   153,   228,   139,   176,   228,   140,   184,\n",
              "         29871,   228,   139,   159,   228,   142,   182,   228,   142,   144,\n",
              "         29871,   228,   139,   139,   228,   140,   137, 29871,   228,   139,\n",
              "           155,   228,   142,   144,   228,   143,   166,   228,   140,   180,\n",
              "         29871,   228,   140,   179,   228,   143,   139,   228,   139,   139,\n",
              "           228,   144,   131,   228,   141,   163,   228,   144,   144,   228,\n",
              "           139,   173,   228,   141,   174, 29871,   228,   139,   136,   228,\n",
              "           140,   168,   228,   139,   171,   228,   140,   184, 29871,   228,\n",
              "           139,   157,   228,   139,   192,   228,   141,   152, 29871,   228,\n",
              "           140,   163,   228,   139,   155,   228,   143,   171,   228,   139,\n",
              "           171,   228,   139,   190, 29871,   228,   142,   171,   228,   142,\n",
              "           139,   228,   143,   166,   228,   140,   180,   228,   141,   152,\n",
              "         29871,   228,   142,   184,   228,   139,   160,   228,   144,   136,\n",
              "         29871,   228,   139,   179,   228,   139,   160,   228,   140,   185,\n",
              "         29871,   228,   140,   168,   228,   139,   135,   228,   139,   174,\n",
              "           228,   142,   141,   228,   142,   144, 29871,   228,   139,   184,\n",
              "           228,   140,   182,   228,   142,   181,   228,   142,   171,   228,\n",
              "           139,   160, 29871,   228,   141,   168,   228,   141,   152,   228,\n",
              "           142,   179,   228,   143,   139,   228,   141,   150, 29871,   228,\n",
              "           139,   139,   228,   139,   184,   228,   144,   153,   228,   139,\n",
              "           176,   228,   140,   180, 29871,   228,   141,   168,   228,   141,\n",
              "           152,   228,   140,   136,   228,   139,   184,   228,   140,   134,\n",
              "           228,   139,   183, 29871,   228,   141,   168,   228,   141,   152,\n",
              "           228,   142,   181,   228,   142,   144,   228,   139,   144, 29871,\n",
              "           228,   139,   159,   228,   142,   182,   228,   142,   144,   228,\n",
              "           141,   152, 29871,   228,   140,   163,   228,   139,   155,   228,\n",
              "           139,   144,   228,   140,   131,   228,   140,   132, 29871,   228,\n",
              "           141,   168,   228,   143,   136,   228,   143,   144, 29871,   228,\n",
              "           140,   179,   228,   142,   179,   228,   139,   184,   228,   140,\n",
              "           179,   228,   141,   150,   228,   139,   144, 29871,   228,   139,\n",
              "           181,   228,   139,   140, 29871,   228,   142,   171,   228,   142,\n",
              "           139,   228,   143,   166,   228,   140,   185,   228,   140,   192,\n",
              "           228,   141,   150, 29871,   228,   139,   184,   228,   144,   153,\n",
              "           228,   139,   176,   228,   140,   184, 29871,   228,   139,   157,\n",
              "           228,   141,   149,   228,   139,   184,   228,   140,   184,   228,\n",
              "           139,   178, 29871,   228,   141,   171,   228,   139,   131,   228,\n",
              "           142,   181,   228,   143,   137, 29871,   228,   139,   161,   228,\n",
              "           139,   131]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
              " 'labels': tensor(1)}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_datasets['train'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6a9fe9f473a24e91a575f2bcd3e8a4b9",
            "35ac89a9aca641fe930e7b50fdf96e36"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-05-30T14:24:07.090970Z",
          "iopub.status.busy": "2024-05-30T14:24:07.090583Z",
          "iopub.status.idle": "2024-05-30T14:24:10.156559Z",
          "shell.execute_reply": "2024-05-30T14:24:10.155296Z",
          "shell.execute_reply.started": "2024-05-30T14:24:07.090934Z"
        },
        "id": "sdlDkDQSiJoE",
        "outputId": "6af71da9-2366-4163-a583-295cbfdad6b6",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:19<00:00,  9.90s/it]\n",
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.44s/it]\n",
            "Some weights of Phi3ForSequenceClassification were not initialized from the model checkpoint at microsoft/Phi-3-mini-128k-instruct and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load the model\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# roberta-base\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    checkpoint,\n",
        "    num_labels=len(category),\n",
        "    id2label = {i: lbl for i, lbl in enumerate(category)},\n",
        "    label2id = {lbl: i for i, lbl in enumerate(category)},\n",
        "    device_map=\"cuda\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-30T14:24:10.161828Z",
          "iopub.status.busy": "2024-05-30T14:24:10.161388Z",
          "iopub.status.idle": "2024-05-30T14:24:10.224241Z",
          "shell.execute_reply": "2024-05-30T14:24:10.223476Z",
          "shell.execute_reply.started": "2024-05-30T14:24:10.161791Z"
        },
        "id": "1z1onqOfiJoG",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=checkpoint+\"-finetuned\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=epochs,\n",
        "    weight_decay=0.1,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    fp16=True,\n",
        "    seed=42,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-30T15:16:46.595651Z",
          "iopub.status.busy": "2024-05-30T15:16:46.595273Z",
          "iopub.status.idle": "2024-05-30T15:16:48.597991Z",
          "shell.execute_reply": "2024-05-30T15:16:48.596975Z",
          "shell.execute_reply.started": "2024-05-30T15:16:46.595619Z"
        },
        "id": "6FvPjfgyiJoI",
        "outputId": "532a6c38-c23f-4a22-b71d-f0f21285860e",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "  metric1 = evaluate.load(\"accuracy\")\n",
        "  metric2 = evaluate.load(\"precision\")\n",
        "  metric3 = evaluate.load(\"recall\")\n",
        "  metric4 = evaluate.load(\"f1\")\n",
        "\n",
        "  logits, labels = eval_preds\n",
        "  predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "  accuracy = metric1.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "  precision = metric2.compute(predictions=predictions, references=labels, average='weighted')[\"precision\"]\n",
        "  recall = metric3.compute(predictions=predictions, references=labels, average='weighted')[\"recall\"]\n",
        "  f1 = metric4.compute(predictions=predictions, references=labels, average='weighted')[\"f1\"]\n",
        "\n",
        "  return {\n",
        "      \"accuracy\": accuracy,\n",
        "      \"precision\": precision,\n",
        "      \"recall\": recall,\n",
        "      \"f1\": f1\n",
        "  }\n",
        "\n",
        "compute_metrics(([[1,0], [0,1]], [0,1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-30T15:16:54.683357Z",
          "iopub.status.busy": "2024-05-30T15:16:54.682462Z",
          "iopub.status.idle": "2024-05-30T15:50:06.480852Z",
          "shell.execute_reply": "2024-05-30T15:50:06.479563Z",
          "shell.execute_reply.started": "2024-05-30T15:16:54.683302Z"
        },
        "id": "E2yiwdpxiJoK",
        "outputId": "54250398-5319-4951-d462-7ec2c0956e00",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-30T15:50:26.786895Z",
          "iopub.status.busy": "2024-05-30T15:50:26.786129Z",
          "iopub.status.idle": "2024-05-30T15:51:03.314295Z",
          "shell.execute_reply": "2024-05-30T15:51:03.312869Z",
          "shell.execute_reply.started": "2024-05-30T15:50:26.786857Z"
        },
        "id": "cuRDqxtHiJoL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Load metrics and evaluate the model\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "eval_dataset = tokenized_datasets[\"test\"].remove_columns([\n",
        "    'article', 'category', 'word_count'\n",
        "    ]).with_format(\"torch\")\n",
        "\n",
        "\n",
        "\n",
        "eval_dataloader = DataLoader(\n",
        "    eval_dataset,\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "y_test, y_pred = [], []\n",
        "model.eval()\n",
        "for batch in eval_dataloader:\n",
        "    batch = {k: v.to('cuda') for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    y_pred.extend(predictions.cpu().numpy())\n",
        "    y_test.extend(batch[\"labels\"].cpu().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-30T15:51:26.898857Z",
          "iopub.status.busy": "2024-05-30T15:51:26.898391Z",
          "iopub.status.idle": "2024-05-30T15:51:26.909514Z",
          "shell.execute_reply": "2024-05-30T15:51:26.907789Z",
          "shell.execute_reply.started": "2024-05-30T15:51:26.898823Z"
        },
        "id": "aADOOG8wiJoM",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(y_pred, y_test):\n",
        "  metric1 = evaluate.load(\"accuracy\")\n",
        "  metric2 = evaluate.load(\"precision\")\n",
        "  metric3 = evaluate.load(\"recall\")\n",
        "  metric4 = evaluate.load(\"f1\")\n",
        "\n",
        "  #logits, labels = y_preds\n",
        "  #predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "  accuracy = metric1.compute(predictions=y_pred, references=y_test)[\"accuracy\"]\n",
        "  precision = metric2.compute(predictions=y_pred, references=y_test, average='weighted')[\"precision\"]\n",
        "  recall = metric3.compute(predictions=y_pred, references=y_test, average='weighted')[\"recall\"]\n",
        "  f1 = metric4.compute(predictions=y_pred, references=y_test, average='weighted')[\"f1\"]\n",
        "\n",
        "  return {\n",
        "      \"accuracy\": accuracy,\n",
        "      \"precision\": precision,\n",
        "      \"recall\": recall,\n",
        "      \"f1\": f1\n",
        "  }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDib-iggiJr5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-30T15:03:31.915215Z",
          "iopub.status.busy": "2024-05-30T15:03:31.914398Z",
          "iopub.status.idle": "2024-05-30T15:03:35.144089Z",
          "shell.execute_reply": "2024-05-30T15:03:35.142956Z",
          "shell.execute_reply.started": "2024-05-30T15:03:31.915180Z"
        },
        "id": "gLela_lIiJr_",
        "outputId": "024dde76-0421-4097-9b55-2a0bfad7b679",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.875232173140596,\n",
              " 'precision': 0.8749618364453737,\n",
              " 'recall': 0.875232173140596,\n",
              " 'f1': 0.8748733706909614}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compute_metrics(y_pred, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-30T15:51:38.585611Z",
          "iopub.status.busy": "2024-05-30T15:51:38.585188Z",
          "iopub.status.idle": "2024-05-30T15:51:42.154374Z",
          "shell.execute_reply": "2024-05-30T15:51:42.153158Z",
          "shell.execute_reply.started": "2024-05-30T15:51:38.585576Z"
        },
        "id": "dr6dwyk_iJsB",
        "outputId": "b43f8583-c2e5-4a47-b4ac-17af5913b0fb",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.875232173140596,\n",
              " 'precision': 0.8749618364453737,\n",
              " 'recall': 0.875232173140596,\n",
              " 'f1': 0.8748733706909614}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compute_metrics(y_pred, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-30T15:11:45.222966Z",
          "iopub.status.busy": "2024-05-30T15:11:45.222582Z",
          "iopub.status.idle": "2024-05-30T15:11:46.213107Z",
          "shell.execute_reply": "2024-05-30T15:11:46.212082Z",
          "shell.execute_reply.started": "2024-05-30T15:11:45.222936Z"
        },
        "id": "GjfUndBciJsD",
        "outputId": "eaedc264-a8b3-455b-9715-a3c85a968f0f",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'f1': 0.8748733706909614}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metric = evaluate.load(\"f1\")\n",
        "metric.compute(predictions=y_pred, references=y_test, average='weighted')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 3141921,
          "sourceId": 5495819,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30699,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
